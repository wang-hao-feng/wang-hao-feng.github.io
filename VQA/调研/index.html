<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><meta name="theme-color" content="#222" media="(prefers-color-scheme: light)"><meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/avatar.jpg"><link rel="icon" type="image/png" sizes="32x32" href="/images/avatar_32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/avatar_16x16.png"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/themes/white/pace-theme-center-circle.css"><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script><script class="next-config" data-name="main" type="application/json">{"hostname":"wang-hao-feng.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":true,"version":"8.9.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script><meta name="description" content="ACL2022  Hypergraph Transformer: Weakly-Supervised Multi-hop Reasoning for Knowledge-based Visual Question Answering 任务：基于知识库的VQA 难点：  需要多跳推理 没有数据监督推理过程的学习  方法分类：  基于memory的方法：以memory的形式表示知识，使用时与q"><meta property="og:type" content="article"><meta property="og:title" content="调研"><meta property="og:url" content="https://wang-hao-feng.github.io/VQA/%E8%B0%83%E7%A0%94/index.html"><meta property="og:site_name" content="匿名甩尸"><meta property="og:description" content="ACL2022  Hypergraph Transformer: Weakly-Supervised Multi-hop Reasoning for Knowledge-based Visual Question Answering 任务：基于知识库的VQA 难点：  需要多跳推理 没有数据监督推理过程的学习  方法分类：  基于memory的方法：以memory的形式表示知识，使用时与q"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://wang-hao-feng.github.io/images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/Hypergraph_Transformer.jpg"><meta property="og:image" content="https://wang-hao-feng.github.io/images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/Hypergraph_Tramsformer_KVQA%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.jpg"><meta property="og:image" content="https://wang-hao-feng.github.io/images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/Hypergraph_Tramsformer_PQ%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.jpg"><meta property="og:image" content="https://wang-hao-feng.github.io/images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/Hypergraph_Tramsformer_%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C.jpg"><meta property="og:image" content="https://wang-hao-feng.github.io/images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/PMR%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.jpg"><meta property="og:image" content="https://wang-hao-feng.github.io/images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/PMR%E9%80%89%E9%A1%B9%E5%88%86%E5%B8%83.jpg"><meta property="og:image" content="https://wang-hao-feng.github.io/images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/TMEG%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84.jpg"><meta property="og:image" content="https://wang-hao-feng.github.io/images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/TMEG%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.jpg"><meta property="og:image" content="https://wang-hao-feng.github.io/images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/clip_nlu.jpg"><meta property="og:image" content="https://wang-hao-feng.github.io/images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/VQA%20zero-shot%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.jpg"><meta property="og:image" content="https://wang-hao-feng.github.io/images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/VE%20zero-shot%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.jpg"><meta property="og:image" content="https://wang-hao-feng.github.io/images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/VQA%20few-shot%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.jpg"><meta property="og:image" content="https://wang-hao-feng.github.io/images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/CLIP%20few-shot%E6%96%B9%E6%B3%95%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.jpg"><meta property="og:image" content="https://wang-hao-feng.github.io/images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/IMAGECODE%E5%AE%9E%E9%AA%8C%E6%A8%A1%E5%9E%8B.jpg"><meta property="og:image" content="https://wang-hao-feng.github.io/images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/IMAGECODE%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.jpg"><meta property="og:image" content="https://wang-hao-feng.github.io/images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/%E4%BD%8D%E7%BD%AE%E5%85%B3%E7%B3%BB%E9%A2%84%E6%B5%8B%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.jpg"><meta property="og:image" content="https://wang-hao-feng.github.io/images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/%E4%BD%8D%E7%BD%AE%E5%85%B3%E7%B3%BB%E9%A2%84%E6%B5%8B%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.jpg"><meta property="article:published_time" content="2022-09-05T06:36:56.000Z"><meta property="article:modified_time" content="2022-09-24T16:11:15.411Z"><meta property="article:author" content="匿名甩尸"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://wang-hao-feng.github.io/images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/Hypergraph_Transformer.jpg"><link rel="canonical" href="https://wang-hao-feng.github.io/VQA/%E8%B0%83%E7%A0%94/"><script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://wang-hao-feng.github.io/VQA/%E8%B0%83%E7%A0%94/","path":"VQA/调研/","title":"调研"}</script><script class="next-config" data-name="calendar" type="application/json">""</script><title>调研 | 匿名甩尸</title><noscript><link rel="stylesheet" href="/css/noscript.css"></noscript></head><body itemscope="" itemtype="http://schema.org/WebPage" class="use-motion"><div class="headband"></div><main class="main"><header class="header" itemscope="" itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><i class="logo-line"></i><p class="site-title">匿名甩尸</p><i class="logo-line"></i></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" maxlength="80" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close" role="button"><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class="search-result-icon"><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div><aside class="sidebar"><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class="sidebar-nav"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="sidebar-panel-container"><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#acl2022"><span class="nav-number">1.</span> <span class="nav-text">ACL2022</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#hypergraph-transformer-weakly-supervised-multi-hop-reasoning-for-knowledge-based-visual-question-answering"><span class="nav-number">1.1.</span> <span class="nav-text">Hypergraph Transformer: Weakly-Supervised Multi-hop Reasoning for Knowledge-based Visual Question Answering</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#premise-based-multimodal-reasoning-conditional-inference-on-joint-textual-and-visual-clues"><span class="nav-number">1.2.</span> <span class="nav-text">Premise-based Multimodal Reasoning: Conditional Inference on Joint Textual and Visual Clues</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#modeling-temporal-modal-entity-graph-for-procedural-multimodal-machine-comprehension"><span class="nav-number">1.3.</span> <span class="nav-text">Modeling Temporal-Modal Entity Graph for Procedural Multimodal Machine Comprehension</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#clip-models-are-few-shot-learners-empirical-studies-on-vqa-and-visual-entailment"><span class="nav-number">1.4.</span> <span class="nav-text">CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual Entailment</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#image-retrieval-from-contextual-descriptions"><span class="nav-number">1.5.</span> <span class="nav-text">Image Retrieval from Contextual Descriptions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#things-not-written-in-text-exploring-spatial-commonsense-from-visual-signals"><span class="nav-number">1.6.</span> <span class="nav-text">Things not Written in Text: Exploring Spatial Commonsense from Visual Signals</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="匿名甩尸" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">匿名甩尸</p><div class="site-description" itemprop="description">个人博客</div></div><div class="site-state-wrap site-overview-item animated"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">8</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">9</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">10</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author site-overview-item animated"><span class="links-of-author-item"><a href="https://github.com/wang-hao-feng" title="GitHub → https://github.com/wang-hao-feng" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a></span></div></div></div></div></aside><div class="sidebar-dimmer"></div></header><div class="back-to-top" role="button" aria-label="返回顶部"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a role="button" class="book-mark-link book-mark-link-fixed"></a><noscript><div class="noscript-warning">Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class="post-block"><article itemscope="" itemtype="http://schema.org/Article" class="post-content" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://wang-hao-feng.github.io/VQA/%E8%B0%83%E7%A0%94/"><span hidden="" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="匿名甩尸"><meta itemprop="description" content="个人博客"></span><span hidden="" itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="匿名甩尸"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">调研</h1><div class="post-meta-container"><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2022-09-05 14:36:56" itemprop="dateCreated datePublished" datetime="2022-09-05T14:36:56+08:00">2022-09-05</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2022-09-25 00:11:15" itemprop="dateModified" datetime="2022-09-25T00:11:15+08:00">2022-09-25</time></span></div></div></header><div class="post-body" itemprop="articleBody"><span id="more"></span><h1 id="acl2022"><a class="markdownIt-Anchor" href="#acl2022"></a> ACL2022</h1><h2 id="hypergraph-transformer-weakly-supervised-multi-hop-reasoning-for-knowledge-based-visual-question-answering"><a class="markdownIt-Anchor" href="#hypergraph-transformer-weakly-supervised-multi-hop-reasoning-for-knowledge-based-visual-question-answering"></a> Hypergraph Transformer: Weakly-Supervised Multi-hop Reasoning for Knowledge-based Visual Question Answering</h2><p>任务：基于知识库的VQA</p><p>难点：</p><ol><li>需要多跳推理</li><li>没有数据监督推理过程的学习</li></ol><p>方法分类：</p><ul><li>基于memory的方法：以memory的形式表示知识，使用时与query计算注意力得分，使用得分高的进行推理</li><li>基于图的方法：在知识库中检索出问题相关的知识并推理（本文方法）</li></ul><p>问题：过度平滑，多跳推理过程中，信息反复传递导致相连的节点特征过于相似</p><p>方法：</p><ol><li>实体链接：将问题和图片中的实体与知识库中链接</li><li>构造问题超图（一条链）和问题启发的知识超图</li><li>利用attention机制进行推理</li></ol><p><img src="../../../images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/Hypergraph_Transformer.jpg" alt=""></p><p>Benchmark:</p><ol><li>KVQA：关注图片中命名实体的VQA数据集，所需的知识区别于常识</li><li>FVQA：提供常识的VQA数据集</li><li>PQ和PQ-Large：用于测试纯文本的多跳推理能力</li></ol><p><img src="../../../images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/Hypergraph_Tramsformer_KVQA%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.jpg" alt="KVQA实验结果"><br><img src="../../../images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/Hypergraph_Tramsformer_PQ%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.jpg" alt="PQ实验结果"></p><p>消融实验：<br><img src="../../../images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/Hypergraph_Tramsformer_%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C.jpg" alt="消融实验结果"><br>SA: self-attention, GA：guided-attention</p><h2 id="premise-based-multimodal-reasoning-conditional-inference-on-joint-textual-and-visual-clues"><a class="markdownIt-Anchor" href="#premise-based-multimodal-reasoning-conditional-inference-on-joint-textual-and-visual-clues"></a> Premise-based Multimodal Reasoning: Conditional Inference on Joint Textual and Visual Clues</h2><p>价值：现有的多模态推理数据集是静态的，即无条件的，但现实世界是复杂的：同样的场景，给定不同的前提会带来不同的结果。<br>工作缺点：这个数据集给的例子中图像是完全可以去除的，图像与回答之间只是匹配的关系，不存在推理或者先验知识。</p><p>提出了一个新数据集PMR：<br>除了图像外，还给了一个描述图片背景的“前提”，预测后续会发生的事件，四选一<br>缺少背景不影响回答事实类问题，但无法复杂问题带来的模棱两可的问题</p><p>对抗样本：从其他问题中选取相似但不正确的选项与正确答案组合<br>实验结果:<br><img src="../../../images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/PMR%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.jpg" alt="PMR实验结果"><br><img src="../../../images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/PMR%E9%80%89%E9%A1%B9%E5%88%86%E5%B8%83.jpg" alt="PMR选项分布"></p><h2 id="modeling-temporal-modal-entity-graph-for-procedural-multimodal-machine-comprehension"><a class="markdownIt-Anchor" href="#modeling-temporal-modal-entity-graph-for-procedural-multimodal-machine-comprehension"></a> Modeling Temporal-Modal Entity Graph for Procedural Multimodal Machine Comprehension</h2><p>任务：多模态程序文件阅读理解</p><p>价值：看待问题的角度。之前的方法都是直接把每一步的图像和文本一起编码，这种方法忽略了程序文件的核心：实体。程序文件可以看作实体变化的组合。</p><p>数据集：</p><ol><li>RecipeQA 关于食谱的问答数据集</li><li>CraftQA 本文提出的一个数据集，关于教程的问答数据集</li></ol><p>方法：TMEG<br>构建一个实体级别的时序图，再将图和节点编码送入模型中<br><img src="../../../images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/TMEG%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84.jpg" alt="TMEG模型结构"><br><img src="../../../images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/TMEG%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.jpg" alt="TMEG实验结果"></p><h2 id="clip-models-are-few-shot-learners-empirical-studies-on-vqa-and-visual-entailment"><a class="markdownIt-Anchor" href="#clip-models-are-few-shot-learners-empirical-studies-on-vqa-and-visual-entailment"></a> CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual Entailment</h2><p>任务：zero-shot/few-show视觉文本理解，包括VQA和VE</p><p>价值：巧妙的将clip的图文匹配能力使用在了NLU任务上<br><img src="../../../images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/clip_nlu.jpg" alt="clip_nlu"></p><p>VQA zero-shot难点：如何减小预训练任务与问答任务之间的差距</p><ol><li>自动将问题传化成填空模板，包括使用T5和依存分析两种方法</li><li>使用T5填空，生成候选答案</li></ol><p>测试跨模态迁移能力：训练时将图像替换成文本，推理时仍采用图像文本对<br>实验发现微调bias和norm层参数比Frozen方法更好</p><p>数据集：<br>VQAv2、SNLI-VE</p><p>实验结果：<br><img src="../../../images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/VQA%20zero-shot%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.jpg" alt="VQA zero-shot"><br><img src="../../../images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/VE%20zero-shot%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.jpg" alt="VE zero-shot"><br><img src="../../../images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/VQA%20few-shot%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.jpg" alt="VQA few-shot"><br><img src="../../../images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/CLIP%20few-shot%E6%96%B9%E6%B3%95%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.jpg" alt="CLIP few-shot"></p><h2 id="image-retrieval-from-contextual-descriptions"><a class="markdownIt-Anchor" href="#image-retrieval-from-contextual-descriptions"></a> Image Retrieval from Contextual Descriptions</h2><p>任务：使用上下文描述检索图像(IMAGECODE)<br>  给定一个描述文本和一些只在细节上有差距的图片，从中选取最符合描述的一个。</p><p>模型需要的能力：</p><ol><li>注意图片中的细节，每条数据的图片都只在细节上有差距</li><li>识别说话人的意图，文本的字面意思可能符合多张图片，但符合说话人意图的只有一张</li><li>时序信息，从视频中选取的图片需要时序信息加以区分</li></ol><p>实验结论：</p><ol><li>静态图片的正确率远高于视频图片</li><li>所有最先进的模型效果都不好</li></ol><p><img src="../../../images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/IMAGECODE%E5%AE%9E%E9%AA%8C%E6%A8%A1%E5%9E%8B.jpg" alt="IMAGECODE实验模型"><br><img src="../../../images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/IMAGECODE%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.jpg" alt="IMAGECODE实验结果"></p><h2 id="things-not-written-in-text-exploring-spatial-commonsense-from-visual-signals"><a class="markdownIt-Anchor" href="#things-not-written-in-text-exploring-spatial-commonsense-from-visual-signals"></a> Things not Written in Text: Exploring Spatial Commonsense from Visual Signals</h2><p>价值：空间位置信息是人们理解世界的一个重要部分，例如：更近的距离意味着更紧密的关系（不同阵营往往在图片左右两端）、更大的物体往往意味着更强。然而，现有的预训练语言模型并不擅长空间相关的任务，因为语言很少包含明确的空间信息，比如不会有人提及骑车的人在车的上方。<br>  本文提出了一个banchmark用于衡量模型的空间感知能力</p><p>任务：</p><ol><li>比较物体大小</li><li>识别特殊动作下人和其他物体的位置关系</li></ol><p>数据集：作者构造的，没写来源</p><p>实验结果：<br><img src="../../../images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/%E4%BD%8D%E7%BD%AE%E5%85%B3%E7%B3%BB%E9%A2%84%E6%B5%8B%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.jpg" alt="位置关系实验结果"><br><img src="../../../images/%E8%AE%BA%E6%96%87/VQA/%E8%B0%83%E7%A0%94/%E4%BD%8D%E7%BD%AE%E5%85%B3%E7%B3%BB%E9%A2%84%E6%B5%8B%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.jpg" alt="比大小实验结果"></p></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/LayoutLMv3%E7%BB%93%E6%9E%84/" rel="prev" title="LayoutLMv3结构"><i class="fa fa-chevron-left"></i> LayoutLMv3结构</a></div><div class="post-nav-item"></div></div></footer></article></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">© <span itemprop="copyrightYear">2022</span> <span class="with-love"><i class="bug"></i> </span><span class="author" itemprop="copyrightHolder">true</span></div><div><span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><script>var now=new Date;function createtime(){var n=new Date("01/25/2022 21:00:00");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="本站已安全运行 "+dnum+" 天 ",document.getElementById("times").innerHTML=hnum+" 小时 "+mnum+" 分 "+snum+" 秒"}setInterval("createtime()",250)</script></div></div></footer><script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script><script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script><script src="/js/third-party/search/local-search.js"></script><script src="/js/third-party/pace.js"></script><script class="next-config" data-name="enableMath" type="application/json">true</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" integrity="sha256-9/mhQQwkpU5okPfM5l0v3LnP9xtc6JK8dKW0/WlGaUc=" crossorigin="anonymous"><script type="text/javascript" charset="utf-8" src="/js/lazyload-plugin/lazyload.intersectionObserver.min.js"></script></body></html>